import sys
import os
import json
import time
import random
import sqlite3
import pandas as pd
import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import undetected_chromedriver as uc
from fake_useragent import UserAgent


class FontParser:
    def __init__(self):
        """åˆå§‹åŒ–å­—ä½“è§£æå™¨åŠå­—ç¬¦æ˜ å°„å…³ç³»"""
        self.mapping = {
            "\ue031": "0",
            "\ue032": "1",
            "\ue033": "2",
            "\ue034": "3",
            "\ue035": "4",
            "\ue036": "5",
            "\ue037": "6",
            "\ue038": "7",
            "\ue039": "8",
            "\ue03a": "9"
        }
    
    def update_mapping(self, new_mapping):
        """æ›´æ–°æ˜ å°„å…³ç³»"""
        self.mapping.update(new_mapping)
        return f"å·²æ›´æ–°æ˜ å°„å…³ç³»ï¼Œå½“å‰æ˜ å°„æ•°: {len(self.mapping)}"
    
    def parse_text(self, text):
        """
        è§£ææ–‡æœ¬å¹¶è¿”å›ç»“æœ
        :param text: å¾…è§£æçš„æ–‡æœ¬
        :return: åŒ…å«åŸå§‹æ–‡æœ¬ã€è§£æç»“æœå’Œåˆ†æä¿¡æ¯çš„å­—å…¸
        """
        if not text:
            return {"original_text": "", "parsed_result": "", "analysis": []}
        
        # è®¡ç®—è§£æç»“æœ
        parsed_result = ''.join([self.mapping.get(char, char) for char in text])
        
        # ç”Ÿæˆè¯¦ç»†åˆ†æä¿¡æ¯
        analysis = []
        for char in text:
            code = f"\\u{ord(char):x}"
            mapped_value = self.mapping.get(char, "æœªæ˜ å°„")
            analysis.append({
                "char": char,
                "unicode": code,
                "mapped_value": mapped_value
            })
        
        # è¿”å›ç»“æœå­—å…¸
        result = {
            "original_text": text,
            "parsed_result": parsed_result,
            "analysis": analysis
        }
        
        # æ‰“å°è§£æç»“æœï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        # self._print_result(result)
        
        return result
    
    def _print_result(self, result):
        """å†…éƒ¨æ–¹æ³•ï¼šæ‰“å°ç»“æœæ ¼å¼åŒ–æ‰“å°"""
        print("=" * 50)
        print(f"åŸå§‹å­—ç¬¦: {result['original_text']}")
        print(f"è§£æç»“æœ: {result['parsed_result']}\n")
        
        print("å­—ç¬¦åˆ†æè¯¦æƒ…:")
        for idx, info in enumerate(result['analysis']):
            print(f"  å­—ç¬¦ {idx + 1}: {info['char']}")
            print(f"    Unicodeç¼–ç : {info['unicode']}")
            print(f"    æ˜ å°„å€¼: {info['mapped_value']}")
        print("=" * 50 + "\n")


class BossJobScraper:
    # ç±»å˜é‡ï¼šè·¯å¾„é…ç½®
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    # STATE_FILE = "login_data.json"  # å¼€å‘
    STATE_FILE = "login_data.json"  # ç”Ÿäº§

    CHROME_BINARY_PATH = os.path.join(CURRENT_DIR, "chrome114-win64", "chrome.exe")
    CHROMEDRIVER_PATH = os.path.join(CURRENT_DIR, "chrome114-win64", "chromedriver.exe")

    def __init__(self, url, query_type, city):
        super().__init__()
        self.url = url
        self.query_type = query_type
        self.city = city
        self.parser = FontParser()

    def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        ua = UserAgent(platforms=["pc"])
        user_agent = ua.random
        print("ğŸ² ä½¿ç”¨çš„ User-Agentï¼š", user_agent)

        chrome_options = Options()
        chrome_options.binary_location = self.CHROME_BINARY_PATH
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument(f"--user-agent={user_agent}")

        # ä½¿ç”¨ undetected-chromedriver
        driver = uc.Chrome(
            driver_executable_path=self.CHROMEDRIVER_PATH,
            options=chrome_options,
            version_main=114,  # æ ¹æ®ä½ çš„ Chrome ç‰ˆæœ¬ä¿®æ”¹
        )

        # éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
            delete navigator.__proto__.webdriver;
            window.chrome = {runtime: {}};
            Object.defineProperty(navigator, 'languages', {
              get: () => ['en-US', 'en']
            });
            """
            },
        )

        return driver

    def load_login_state(self):
        """åŠ è½½ç™»å½•çŠ¶æ€ï¼ˆCookies + localStorage + sessionStorageï¼‰"""
        if not os.path.exists(self.STATE_FILE):
            print("âš ï¸ æœªæ‰¾åˆ°ç™»å½•çŠ¶æ€æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ login_boss.py ç™»å½• Boss ç›´è˜ï¼")
            raise Exception("ç™»å½•çŠ¶æ€æ–‡ä»¶ç¼ºå¤±")

        print("ğŸª æ­£åœ¨åŠ è½½ç™»å½•çŠ¶æ€...")
        with open(self.STATE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.driver.get("https://www.zhipin.com/")

        # æ·»åŠ  Cookies
        for cookie in data.get("cookies", []):
            try:
                cookie.pop("sameSite", None)
                cookie.pop("expiry", None)
                cookie.pop("httpOnly", None)
                cookie.pop("secure", None)
                self.driver.add_cookie(cookie)
            except Exception as e:
                print("âš ï¸ æ·»åŠ  Cookie å‡ºé”™ï¼š", e)

        # è®¾ç½® localStorage å’Œ sessionStorage
        self.set_storage(data.get("localStorage", {}), "localStorage")
        self.set_storage(data.get("sessionStorage", {}), "sessionStorage")

        # åˆ·æ–°é¡µé¢ä»¥æ¢å¤ç™»å½•çŠ¶æ€
        self.driver.get(self.url)
        print("âœ… ç™»å½•çŠ¶æ€åŠ è½½å®Œæˆ")

    def set_storage(self, storage_dict, storage_type="localStorage"):
        script = ""
        for key, value in storage_dict.items():
            script += f"window.{storage_type}.setItem('{key}', '{value}');"
        if script:
            self.driver.execute_script(script)

    def create_table(self):
        """åˆ›å»ºæ•°æ®åº“è¡¨"""
        self.conn.execute(
            """CREATE TABLE IF NOT EXISTS jobs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            company TEXT,
            location TEXT,
            salary TEXT,
            experience TEXT,
            education TEXT,
            description TEXT,
            url TEXT UNIQUE,
            query_type TEXT,
            city TEXT,
            channel TEXT,
            created_at TEXT
        )"""
        )
        self.conn.commit()

    def scroll_to_bottom(self, scroll_step=300, max_retries=50, retry_interval=2):
        print("â¬‡ï¸ å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½å²—ä½...")

        job_selector = "ul.rec-job-list .job-card-box a.job-name"
        seen_jobs = set()
        retry = 0

        while retry < max_retries:
            print("ğŸ”„ æ­£åœ¨æ»šåŠ¨åŠ è½½å²—ä½...", max_retries - retry)
            job_elements = self.driver.find_elements(By.CSS_SELECTOR, job_selector)
            current_job_hrefs = [
                j.get_attribute("href") for j in job_elements if j.get_attribute("href")
            ]
            new_jobs = [h for h in current_job_hrefs if h not in seen_jobs]

            if new_jobs:
                seen_jobs.update(current_job_hrefs)
                retry = 0
            else:
                retry += 1

            scroll_step = random.randint(200, 500)
            self.driver.execute_script(f"window.scrollBy(0, {scroll_step});")
            time.sleep(random.uniform(0.5, 1.5))

            pos = self.driver.execute_script(
                "return window.scrollY + window.innerHeight;"
            )
            total = self.driver.execute_script("return document.body.scrollHeight;")
            if pos >= total:
                break

        print("âœ… æ»šåŠ¨åŠ è½½å®Œæˆï¼Œå…±åŠ è½½å²—ä½æ•°é‡ï¼š", len(seen_jobs))
        return list(seen_jobs)

    def extract_jobs(self):
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "ul.rec-job-list"))
        )
        job_items = self.driver.find_elements(
            By.CSS_SELECTOR, "ul.rec-job-list li.job-card-box"
        )
        jobs = []
        for item in job_items:
            try:
                a_tag = item.find_element(By.CSS_SELECTOR, "a.job-name")
                href = a_tag.get_attribute("href")
                job_url = (
                    href if href.startswith("http") else "https://www.zhipin.com" + href
                )
                job_name = a_tag.text.strip()
                salary = item.find_element(
                    By.CSS_SELECTOR, "span.job-salary"
                ).text.strip()
                company = item.find_element(
                    By.CSS_SELECTOR, "span.boss-name"
                ).text.strip()
                location = item.find_element(
                    By.CSS_SELECTOR, "span.company-location"
                ).text.strip()
                tags = [
                    t.text.strip()
                    for t in item.find_elements(By.CSS_SELECTOR, "ul.tag-list li")
                ]
                experience = tags[0] if len(tags) > 0 else ""
                education = tags[1] if len(tags) > 1 else ""

             

                jobs.append(
                    {
                        "title": job_name,
                        "company": company,
                        "location": location,
                        "salary": self.parser.parse_text(salary)['parsed_result'],
                        "experience": experience,
                        "education": education,
                        "description": "",
                        "url": job_url,
                        "query_type": self.query_type,
                        "channel": "Bossç›´è˜",
                        "created_at": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "city": self.city,
                    }
                )
            except Exception as e:
                print("âš ï¸ æå–å¤±è´¥ï¼š", e)
                continue
        return jobs

    def extract_job_description(self, job_element):
        try:
            job_element.click()
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, "div.job-detail-body p.desc")
                )
            )
            description = self.driver.find_element(
                By.CSS_SELECTOR, "div.job-detail-body p.desc"
            ).text.strip()
            labels = [
                e.text.strip()
                for e in self.driver.find_elements(
                    By.CSS_SELECTOR, "ul.job-label-list li"
                )
            ]
            boss_name = self.driver.find_element(
                By.CSS_SELECTOR, "div.job-boss-info h2.name"
            ).text.strip()
            company_and_title = self.driver.find_element(
                By.CSS_SELECTOR, "div.boss-info-attr"
            ).text.strip()
            try:
                job_address = self.driver.find_element(
                    By.CSS_SELECTOR, "p.job-address-desc"
                ).text.strip()
            except:
                job_address = "æ— "
            full_description = f"{description}\n\nå²—ä½æ ‡ç­¾ï¼š{', '.join(labels)}\næ‹›è˜äººï¼š{boss_name}\nå…¬å¸ï¼š{company_and_title}\nå·¥ä½œåœ°å€ï¼š{job_address}"
            return full_description
        except Exception as e:
            print("âš ï¸ æå–èŒä½æè¿°å¤±è´¥ï¼š", e)
            return ""

    def save_single_job(self, job):
        cursor = self.conn.cursor()

        # now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        cursor.execute(
            """INSERT OR IGNORE INTO jobs 
            (title, company, location, salary, experience, education, description, url, query_type, city, channel, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
            (
                job["title"],
                job["company"],
                job["location"],
                job["salary"],
                job["experience"],
                job["education"],
                job["description"],
                job["url"],
                job["query_type"],
                job["city"],
                job["channel"],
                job["created_at"]
            ),
        )
        self.conn.commit()

    def scrape_jobs(self):

        print("ğŸ“ å‡†å¤‡æ•°æ®åº“...")
        self.conn = sqlite3.connect("T:/MyApp/zhaopin-win-amd64/jobs.db")  # ç”Ÿäº§
        self.create_table()
        print("âœ… æ•°æ®åº“å‡†å¤‡å®Œæˆ")

        try:
            print("ğŸŒ æ­£åœ¨åˆå§‹åŒ–æµè§ˆå™¨...")
            self.driver = self.init_browser()
            self.load_login_state()
        except Exception as e:
            print("âš ï¸ åˆå§‹åŒ–æµè§ˆå™¨æ—¶å‘ç”Ÿé”™è¯¯ï¼š", e)
            exit(1)

        """æŠ“å–å²—ä½ä¿¡æ¯"""
        try:
            print("ğŸ” å¼€å§‹çˆ¬å–å²—ä½ä¿¡æ¯...")
            self.driver.maximize_window()
            print(self.url)
            print("ğŸŒ æ­£åœ¨åŠ è½½å²—ä½é¡µé¢...")

            self.driver.get(self.url)
            time.sleep(5)

            self.scroll_to_bottom(scroll_step=300, max_retries=50, retry_interval=2)

            jobs = self.extract_jobs()
            total = 0
            for job in jobs:

                try:
                    job_element = self.driver.find_element(
                        By.XPATH,
                        f"//a[@href='{job['url'].replace('https://www.zhipin.com', '')}']/ancestor::li",
                    )
                    description = self.extract_job_description(job_element)
                    job["description"] = description
                    self.save_single_job(job)
                    total += 1

                except Exception as e:
                    print("âš ï¸ å¤„ç†å²—ä½å¤±è´¥ï¼š", e)
                    continue

                time.sleep(random.randint(1, 3))

            print(f"âœ… å·²ä¿å­˜ {total} æ¡å²—ä½ä¿¡æ¯ï¼ˆå«èŒä½æè¿°ï¼‰")

            print("âœ… æ•°æ®çˆ¬å–å®Œæˆ")

        except Exception as e:
            print("âš ï¸ çˆ¬å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š", e)
            raise e

    def close(self):
        """å…³é—­æµè§ˆå™¨å’Œæ•°æ®åº“è¿æ¥"""
        self.driver.quit()
        self.conn.close()


def main():
    """ä¸»å‡½æ•°"""
    # ç¡¬ç¼–ç çš„URLåˆ—è¡¨å’ŒæŸ¥è¯¢ç±»å‹
    url = "https://www.zhipin.com/web/geek/jobs?city={}&query={}"
    # query_types = ["java", "python", "ç®—æ³•", "å‰ç«¯"]
    query_types = ["æœåŠ¡å‘˜"]

    # city_map={"åŒ—äº¬":"101010100","ä¸Šæµ·":"101020100","æ·±åœ³":"101280600","å¹¿å·":"101280100","æ­¦æ±‰":"101200100"}
    city_map = {"ä¸Šæµ·": "101020100"}
    scrapers = []

    # åŠ¨æ€åˆ›å»ºå¤šä¸ªBossJobScraperå¯¹è±¡
    for city_name, city_code in city_map.items():
        for query_type in query_types:
            scraper_url = url.format(city_code, query_type)
            scraper = BossJobScraper(
                url=scraper_url, query_type=query_type, city=city_name
            )
            scrapers.append(scraper)

    print(f"ğŸš€ å¯åŠ¨Bossç›´è˜çˆ¬è™«ï¼Œå…± {len(scrapers)} ä¸ªä»»åŠ¡...")

    # ä¾æ¬¡æ‰§è¡Œæ¯ä¸ªçˆ¬è™«ä»»åŠ¡
    for i, scraper in enumerate(scrapers):
        print(f"\nğŸ”„ å¼€å§‹å¤„ç†ç¬¬ {i+1}/{len(scrapers)} ä¸ªä»»åŠ¡...")
        try:
            scraper.scrape_jobs()
            time.sleep(random.randint(5,10) * 60)
        except Exception as e:
            print(f"âŒ ç¬¬ {i+1} ä¸ªä»»åŠ¡å‡ºé”™ï¼š{str(e)}")
        finally:
            scraper.close()

    print("âœ… æ‰€æœ‰çˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œæµè§ˆå™¨å·²å…³é—­")


if __name__ == "__main__":
    main()
