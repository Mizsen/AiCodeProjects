import sqlite3
import time
import os
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import undetected_chromedriver as uc
from fake_useragent import UserAgent

import random


# boss_spider.py
# Boss ç›´è˜å²—ä½ä¿¡æ¯æŠ“å–è„šæœ¬

# é…ç½®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
STATE_FILE = "login_data.json"

chrome_binary_path = os.path.join(current_dir, "chrome114-win64", "chrome.exe")
chromedriver_path = os.path.join(current_dir, "chrome114-win64", "chromedriver.exe")


# åˆå§‹åŒ–æµè§ˆå™¨
def init_browser():

    # è®¾ç½® User-Agent
    ua = UserAgent(platforms=["pc"])
    user_agent = ua.random
    print("ğŸ² ä½¿ç”¨çš„ User-Agentï¼š", user_agent)
    chrome_options = Options()
    chrome_options.binary_location = chrome_binary_path
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--start-maximized")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument(f'--user-agent={user_agent}')  # ä½¿ç”¨éšæœº User-Agent


    # service = Service(executable_path=chromedriver_path)
    # driver = webdriver.Chrome(service=service, options=chrome_options)

    # åˆå§‹åŒ– undetected-chromedriver
    driver = uc.Chrome(
        driver_executable_path=chromedriver_path,  # chromedriver è·¯å¾„
        options=chrome_options,
        version_main=114  # æ ¹æ®ä½ æœ¬åœ° Chrome çš„ç‰ˆæœ¬å·å¡«å†™ï¼Œä¾‹å¦‚ 125
    )

    # è¿›ä¸€æ­¥éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
        delete navigator.__proto__.webdriver;
        window.chrome = {runtime: {}};
        Object.defineProperty(navigator, 'languages', {
          get: () => ['en-US', 'en']
        });
        """
    })

    return driver


class BossJobScraper:
    def __init__(self, url):
        self.driver = init_browser()
        self.url = url
        self.conn = sqlite3.connect("../jobs.db")
        self.create_table()
        self.load_login_state()

    def load_login_state(self):
        """åŠ è½½ç™»å½•çŠ¶æ€ï¼ˆCookies + localStorage + sessionStorageï¼‰"""
        if not os.path.exists(STATE_FILE):
            print("âš ï¸ æœªæ‰¾åˆ°ç™»å½•çŠ¶æ€æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ login_boss.py ç™»å½• Boss ç›´è˜ï¼")
            exit()

        print("ğŸª æ­£åœ¨åŠ è½½ç™»å½•çŠ¶æ€...")
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.driver.get("https://www.zhipin.com/")

        # æ·»åŠ  Cookies
        for cookie in data.get("cookies", []):
            try:
                cookie.pop("sameSite", None)
                cookie.pop("expiry", None)
                cookie.pop("httpOnly", None)
                cookie.pop("secure", None)
                self.driver.add_cookie(cookie)
            except Exception as e:
                print("âš ï¸ æ·»åŠ  Cookie å‡ºé”™ï¼š", e)

        # è®¾ç½® localStorage å’Œ sessionStorage
        self.set_storage(data.get("localStorage", {}), "localStorage")
        self.set_storage(data.get("sessionStorage", {}), "sessionStorage")

        # åˆ·æ–°é¡µé¢ä»¥æ¢å¤ç™»å½•çŠ¶æ€
        self.driver.get(self.url)
        print("âœ… ç™»å½•çŠ¶æ€åŠ è½½å®Œæˆ")

    def set_storage(self, storage_dict, storage_type="localStorage"):
        script = ""
        for key, value in storage_dict.items():
            script += f"window.{storage_type}.setItem('{key}', '{value}');"
        if script:
            self.driver.execute_script(script)

    def create_table(self):
        """åˆ›å»ºæ•°æ®åº“è¡¨"""
        self.conn.execute(
            """CREATE TABLE IF NOT EXISTS jobs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            
            title TEXT,
            company TEXT,
            location TEXT,
            salary TEXT,
            experience TEXT,
            education TEXT,
            description TEXT,
            url TEXT UNIQUE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )"""
        )
        self.conn.commit()

    def scroll_to_bottom(self, max_scrolls=20, scroll_pause=5):
        """æ»šåŠ¨åŠ è½½å²—ä½åˆ—è¡¨"""
        print("â¬‡ï¸ å¼€å§‹æ»šåŠ¨åŠ è½½å²—ä½...")
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0

        while scroll_count < max_scrolls:
            self.driver.execute_script(
                "window.scrollTo(0, document.body.scrollHeight);"
            )
            print(f"ğŸ”„ ç¬¬ {scroll_count + 1} æ¬¡æ»šåŠ¨")
            time.sleep(scroll_pause)

            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                print("ğŸ”š é¡µé¢å·²åˆ°åº•ï¼Œåœæ­¢æ»šåŠ¨")
                break

            last_height = new_height
            scroll_count += 1

        print("âœ… æ»šåŠ¨åŠ è½½å®Œæˆ")




    def scroll_to_bottom(self, scroll_step=300, max_retries=50, retry_interval=2):
        print("â¬‡ï¸ å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½å²—ä½...")

        job_selector = "ul.rec-job-list .job-card-box a.job-name"
        seen_jobs = set()
        retry = 0

        while retry < max_retries:
            # è·å–å½“å‰å²—ä½é“¾æ¥
            job_elements = self.driver.find_elements(By.CSS_SELECTOR, job_selector)
            current_job_hrefs = [job.get_attribute('href') for job in job_elements if job.get_attribute('href')]

            new_jobs = [href for href in current_job_hrefs if href not in seen_jobs]

            if new_jobs:
                print(f"ğŸ†• æ–°åŠ è½½ {len(new_jobs)} ä¸ªå²—ä½")
                seen_jobs.update(current_job_hrefs)
                retry = 0
            else:
                print("ğŸ”„ æœªæ£€æµ‹åˆ°æ–°å²—ä½ï¼Œç»§ç»­æ»šåŠ¨")
                retry += 1

            # æ»šåŠ¨é¡µé¢
            scroll_step = random.randint(200, 500)  # éšæœºæ­¥é•¿
            self.driver.execute_script(f"window.scrollBy(0, {scroll_step});")

            # éšæœºç­‰å¾…æ—¶é—´
            time.sleep(random.uniform(1, 3))

            # æ£€æŸ¥æ˜¯å¦åˆ°åº•
            current_position = self.driver.execute_script("return window.scrollY + window.innerHeight;")
            total_height = self.driver.execute_script("return document.body.scrollHeight;")
            if current_position >= total_height:
                print("ğŸ”š å·²æ»šåŠ¨è‡³é¡µé¢åº•éƒ¨")
                break

        print("âœ… æ»šåŠ¨åŠ è½½å®Œæˆï¼Œå…±åŠ è½½å²—ä½æ•°é‡ï¼š", len(seen_jobs))
        return list(seen_jobs)

    def extract_jobs(self):
        """æå–å²—ä½åˆ—è¡¨ä¿¡æ¯"""
        WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "ul.rec-job-list"))
        )

        job_items = self.driver.find_elements(
            By.CSS_SELECTOR, "ul.rec-job-list li.job-card-box"
        )
        jobs = []

        for item in job_items:
            try:
                a_tag = item.find_element(By.CSS_SELECTOR, "a.job-name")
                href = a_tag.get_attribute("href")

                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å®Œæ•´çš„ URL
                if href.startswith("http"):
                    job_url = href
                else:
                    job_url = "https://www.zhipin.com" + href  # è¡¥å…¨ URL

                job_name = a_tag.text.strip()

                salary = item.find_element(
                    By.CSS_SELECTOR, "span.job-salary"
                ).text.strip()
                company_name = item.find_element(
                    By.CSS_SELECTOR, "span.boss-name"
                ).text.strip()
                location = item.find_element(
                    By.CSS_SELECTOR, "span.company-location"
                ).text.strip()

                # æå–æ‰€æœ‰ tag æ ‡ç­¾
                tag_elements = item.find_elements(By.CSS_SELECTOR, "ul.tag-list li")
                tags = [tag.text.strip() for tag in tag_elements]

                # æå–å‰ä¸¤ä¸ªä½œä¸ºç»éªŒ & å­¦å†
                experience = tags[0] if len(tags) > 0 else None
                education = tags[1] if len(tags) > 1 else None

                # å‰©ä¸‹çš„ä½œä¸ºæŠ€èƒ½æ ‡ç­¾
                skills = tags[2:] if len(tags) > 2 else []

                jobs.append(
                    {
                        "title": job_name,
                        "company": company_name,
                        "location": location,
                        "salary": salary,
                        "experience": experience,
                        "education": education,
                        "url": job_url,
                    }
                )
            except Exception as e:
                print("âš ï¸ æå–æŸæ¡æ•°æ®å‡ºé”™ï¼š", e)
                continue

        return jobs

    def extract_job_description(self, job_element):
        """
        ç‚¹å‡»å²—ä½å…ƒç´ ï¼Œç­‰å¾…å³ä¾§èŒä½æè¿°åŒºåŸŸåŠ è½½ï¼Œæå–å†…å®¹
        :param job_element: å²—ä½çš„ WebElement å…ƒç´ 
        :return: å®Œæ•´çš„èŒä½æè¿°ï¼ˆå«æ ‡ç­¾ã€Bossä¿¡æ¯ã€å·¥ä½œåœ°å€ï¼‰
        """
        try:
            # ç‚¹å‡»å²—ä½å¡ç‰‡
            job_element.click()
            print("ğŸ–±ï¸ å·²ç‚¹å‡»å²—ä½å¡ç‰‡ï¼Œç­‰å¾…èŒä½æè¿°åŠ è½½...")

            # ç­‰å¾…èŒä½æè¿°åŒºåŸŸåŠ è½½å®Œæˆ
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.job-detail-body p.desc"))
            )

            # æå–èŒä½æè¿°æ–‡æœ¬
            description = self.driver.find_element(By.CSS_SELECTOR, "div.job-detail-body p.desc").text.strip()

            # æå–å²—ä½æ ‡ç­¾
            label_elements = self.driver.find_elements(By.CSS_SELECTOR, "ul.job-label-list li")
            labels = [label.text.strip() for label in label_elements]
            labels_str = "ï¼Œ".join(labels) if labels else "æ— "

            # æå– Boss ä¿¡æ¯
            boss_name = self.driver.find_element(By.CSS_SELECTOR, "div.job-boss-info h2.name").text.strip()
            company_and_title = self.driver.find_element(By.CSS_SELECTOR, "div.boss-info-attr").text.strip()

            try:
                # æå–å·¥ä½œåœ°å€
                job_address = self.driver.find_element(By.CSS_SELECTOR, "p.job-address-desc").text.strip()
            except Exception as e:
                print("âš ï¸ æå–å·¥ä½œåœ°å€å¤±è´¥ï¼š", e)
                job_address = "æ— "

            # æ„é€ å®Œæ•´æè¿°
            full_description = (
                f"{description}\n\n"
                f"â€”â€”â€”â€”â€”â€”\n"
                f"å²—ä½æ ‡ç­¾ï¼š{labels_str}\n"
                f"æ‹›è˜äººï¼š{boss_name}\n"
                f"å…¬å¸ï¼š{company_and_title}\n"
                f"å·¥ä½œåœ°å€ï¼š{job_address}"
            )

            return full_description

        except Exception as e:
            print("âš ï¸ æå–èŒä½æè¿°å¤±è´¥ï¼š", e)
            return None
    


    def save_single_job(self, job):
        """ä¿å­˜å•ä¸ªå²—ä½ä¿¡æ¯ï¼ˆå«æè¿°ï¼‰"""
        cursor = self.conn.cursor()
        cursor.execute(
            """
            INSERT OR IGNORE INTO jobs (title, company, location, salary, experience, education, description, url)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                job["title"],
                job["company"],
                job["location"],
                job["salary"],
                job["experience"],
                job["education"],
                job.get("description", ""),
                job["url"],
            ),
        )
        self.conn.commit()

    def scrape_jobs(self, max_scrolls=20):
        """ä¸»æŠ“å–æµç¨‹"""
        print("ğŸŒ æ­£åœ¨åŠ è½½å²—ä½é¡µé¢...")
        self.driver.get(self.url)
        time.sleep(5)

        seen_urls = set()
        total_jobs = 0

        # self.scroll_to_bottom(max_scrolls=max_scrolls)
        self.scroll_to_bottom(scroll_step=300, max_retries=50, retry_interval=2)

        jobs = self.extract_jobs()
        if not jobs:
            print("âš ï¸ æ²¡æœ‰å²—ä½ä¿¡æ¯")
            return

        for job in jobs:
            if job["url"] in seen_urls:
                continue

            # æ‰¾åˆ°å¯¹åº”çš„å²—ä½å…ƒç´ 
            try:
                # é‡æ–°æŸ¥æ‰¾å²—ä½å…ƒç´ ï¼ˆé¿å… StaleElementReferenceExceptionï¼‰
                job_element = self.driver.find_element(
                    By.XPATH,
                    f"//a[@href='{job['url'].replace('https://www.zhipin.com', '')}']/ancestor::li",
                )
            except Exception as e:
                print("âš ï¸ æ— æ³•æ‰¾åˆ°å²—ä½å…ƒç´ ï¼Œè·³è¿‡ï¼š", e)
                continue

            description = self.extract_job_description(job_element)
            job["description"] = description

            self.save_single_job(job)
            seen_urls.add(job["url"])
            total_jobs += 1

        print(f"âœ… æ€»å…±ä¿å­˜ {total_jobs} æ¡å²—ä½ä¿¡æ¯ï¼ˆå«èŒä½æè¿°ï¼‰")

    def close(self):
        self.driver.quit()
        self.conn.close()


# =======================
# âœ… ç¤ºä¾‹è¿è¡Œ
# =======================
if __name__ == "__main__":
    target_url = "https://www.zhipin.com/web/geek/job?query=Java&city=101200100"  # å¯æ›¿æ¢ä¸ºä»»æ„å²—ä½é¡µ
    scraper = BossJobScraper(target_url)
    try:
        scraper.scrape_jobs(max_scrolls=5)
    finally:
        scraper.close()

        print("ğŸ‘‹ æµè§ˆå™¨å·²å…³é—­ï¼Œæ•°æ®åº“è¿æ¥å·²å…³é—­")
